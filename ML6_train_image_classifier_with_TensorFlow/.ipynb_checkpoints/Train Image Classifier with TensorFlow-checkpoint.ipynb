{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say you want to build a classifier that can tell the difference between a picture of T-Rex and a triceratops.\n",
    "![dinosuars](./assets/dinosaur.png)\n",
    "\n",
    "Or a painting as being a Monet or a Picasso.\n",
    "![paintins](./assets/art.png)\n",
    "\n",
    "To do this I'm going to work with a codelab called [TensorFlow for Poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0). This is a great way to start learning about and workiing with image classification.\n",
    "* This code lab is high level: to train the classifier I'll basiclly need to run a couple of scripts.\n",
    "* Powerful classifier: although the code lab is high level, but it's impresseive what the classifier can create. It's better than what a google-developer could have written just a couple of years ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train an image classifier with _TensorFlow for Poets_, I'll only need to provide one thing:\n",
    "* Training data\n",
    "\n",
    "In this case, it's just directories full of images. My plan is to create a classifier to tell the difference between five types of flowers: \n",
    "1. Daisy\n",
    "* Dandelion\n",
    "* Roses\n",
    "* Sunflowers\n",
    "* Tulips\n",
    "\n",
    "And here's what the training data looks like. Notice there are five directories; one for each type of flower. If you want to use your own images, say, for dinosaurs, or paintings, all you need to do is create a directory and fill it with images. Each directory needs about 100 images to start.\n",
    "![flower directory](./assets/directory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my Mac, I opened the termial, navigated the project directory and ran the following code:\n",
    "```unix\n",
    "curl http://download.tensorflow.org/example_images/flower_photos.tgz | tar xz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the tarining data, the next thing I need to do is train my classifier. For this, I'll use TensorFlow. TensorFlow is an open source machine learning library, and is especially useful for workind with the branch of machine learning called deep learning.\n",
    "\n",
    "Deep learning has lead to great results in the last couple years. Especially in domains like image classification (which is what I'll be working with today). Here's one reason why: \n",
    "* Recall: In the firstmodule I discussed the difference between apples and oranges. It's impossible to do this by hand because there is too much variation in the world. But, I now know that classififires take featurs as input; and with images it's incredibly hard to extract useful features by hand. For example: you wouldn't want to detect the texture of a piece of fruit. To get aroudn this, I will use deep learning, because it has a major advantage when working with images, and it's this. You don't need to extract features manually. Instead, you can use the raw pixels of the image's features, and the classifier will do the rest. \n",
    "![raw pixels](./assets/rawPixels.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the difference in the training data, let's compare the Iris data set with the directories of images. In Iris, each column is a feature that describest the flower. You can imagine I came up with these features manually, say, by measuring the flower with a ruler.\n",
    "![raw pixels](./assets/iris.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now by contrast, here's the training data in TensorFlow for Poets. It's just a list of labeled images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rose.png</td>\n",
       "      <td>Rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tulipgarden.jpg</td>\n",
       "      <td>Tulip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flowerfield.png</td>\n",
       "      <td>Sunflower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whiteflower.jpg</td>\n",
       "      <td>Daisy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         File Name      Label\n",
       "0         rose.png       Rose\n",
       "1  tulipgarden.jpg      Tulip\n",
       "2  flowerfield.png  Sunflower\n",
       "3  whiteflower.jpg      Daisy\n",
       "4              ...        ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"File Name\":[\"rose.png\",\"tulipgarden.jpg\",\"flowerfield.png\",\"whiteflower.jpg\",\"...\"],\n",
    "              \"Label\":[\"Rose\",\"Tulip\",\"Sunflower\",\"Daisy\",\"...\"]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a classifier is just a fucntion:\n",
    "```python\n",
    "f(x) = y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, _x_ is a 2D array of pixels from the image, and _y_ is a label like rose. Now when talking about deep learning, the classifier we'll be using is called a neural network. \n",
    "![neural network](./assets/neuralNetwork.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, a neuarl network is just antoher type of classifier, like the nearest neighbor I classifier I wrote in the previous module. The difference is a neural network can learn more complex functions.  In this code labe, TensorFlow for poests takes care of setting up and training the neural network behind the scenes. This doesn't mean TensorFlow code is any harder to write than what I've worked with so far. In fact, *[TF Learn](http://tflearn.org/)* is a high level machine learning library on top of TensorFlow, and the syntax is similar to scikit-learn that has been shown so far.\n",
    "\n",
    "For example here's a code snippet that shows you how to import a neural network, train it, and use it to classify data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# this code cell is just hear for example \n",
    "# to show how simple it is to use tensorflow\n",
    "\n",
    "from sklearn import metrics, cross_validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "def main():\n",
    "    iris = learn.datasets.load_dataset('iris')\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n",
    "\n",
    "    # Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "    classifier = learn.DNNClassifier(hidden_units=[10,20,10], n_classes=3, feature_columns=feature_columns)\n",
    "\n",
    "    # # Fit and predict.\n",
    "    classifier.fit(X_train, y_train, steps=200)\n",
    "    score = metrics.accuracy_score(y_test, classifier.predict(X_test))\n",
    "    print('Accuracy: {0:f}'.format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show some parts of this tutorial's code in jupiter notebook, but most will have to be run from the terminal. To work through this tutorial, navigate to [TensorFlow For Poets 2](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0). \n",
    "\n",
    "You can use [Built in magic commands for iPython](https://ipython.readthedocs.io/en/stable/interactive/magics.html) to try running some of the Bash commands within Jupyter Notebook. Here are a few examples:\n",
    "\n",
    "* %pwd\n",
    "    * present working directory\n",
    "\n",
    "\n",
    "* %pushd\n",
    "    * Place the current dir on stack and change directory \n",
    "    * %pushd ./tensorflow-for-poets-2\n",
    "\n",
    "\n",
    "* %popd\n",
    "    * can use this command to Change to directory popped off the top of the stack.\n",
    "\n",
    "\n",
    "* %dirs\n",
    "    * Return the current directory stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, install tensorflow.\n",
    "\n",
    "```python\n",
    "!pip install --upgrade tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the terminal to navigate to directory of choice and clone git repository.\n",
    "```python\n",
    "!git clone https://github.com/googlecodelabs/tensorflow-for-poets-2\n",
    "\n",
    "!cd tensorflow-for-poets-2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up TensorFlow For Poets navigate to the section [(Re)training the network](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/?utm_campaign=chrome_series_machinelearning_063016&utm_source=gdev&utm_medium=yt-desc#3) to train the classifier.\n",
    "\n",
    "To do that start with this script.\n",
    "```SHELL\n",
    "python -m scripts.retrain \\\n",
    "  --bottleneck_dir=tf_files/bottlenecks \\\n",
    "  --how_many_training_steps=500 \\\n",
    "  --model_dir=tf_files/models/ \\\n",
    "  --summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\" \\\n",
    "  --output_graph=tf_files/retrained_graph.pb \\\n",
    "  --output_labels=tf_files/retrained_labels.txt \\\n",
    "  --architecture=\"${ARCHITECTURE}\" \\\n",
    "  --image_dir=tf_files/flower_photos\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download photos by invoking the following command.\n",
    "```SHELL\n",
    "curl http://download.tensorflow.org/example_images/flower_photos.tgz \\\n",
    "    | tar xz -C tf_files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm you have a copy of the flower photos.\n",
    "```python\n",
    "!ls tf_files/flower_photos\n",
    "```\n",
    "\n",
    "The following objects should be displayed:\n",
    "* daisy/\n",
    "* dandelion/\n",
    "* roses/\n",
    "* sunflowers/\n",
    "* tulip/\n",
    "* LICENSE.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands are recommended to obtimize the retraining of MobileNet model. Input image resolution will be set to 24 pixels, and use 0.5 to set the relative size of the model as a fraction of the largest MobileNet.\n",
    "\n",
    "```SHELL\n",
    "IMAGE_SIZE=224\n",
    "ARCHITECTURE=\"mobilenet_0.50_${IMAGE_SIZE}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of mobile net is further explained in this graph and the following paragraph.\n",
    "\n",
    "![test](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/img/70170cbb89d318b1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure is further explained in [slides 84-89](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf).\n",
    "\n",
    "Video lecture covering the slides, starting around [1hr 4m](https://www.youtube.com/watch?v=DAOcjicFr1Y).\n",
    "\n",
    "This graph can be found displays the compultational complexity of 4 Deep Neural Network Models. The y-axis displays Top-1 Accuracy; where higher is betterr. The x-axis displays the number of operations, so the further to the right, the more ops that are being run (i.e. more computationally expensive; the bigger the circle the more memory usage). So we can see that VGG 16 (grey circle) is the least efficient when compared to MobileNet's 16 points (green dots). VGG 16 uses the most memory, and has the most operations on this graph (not saying this is undesirable, just more expensive in terms of memory). For example, Look at the point in the lower left of graph. This is a MobileNet model; although it is uses less operations and uses less memory (small circle size), it has the lowest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the training, launch tensorboard in the background. TensorBoard is a monitoring and inspection tool included with tensorflow. You will use it to monitor the training progress.\n",
    "\n",
    "```SHELL\n",
    "tensorboard --logdir tf_files/training_summaries &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this python command to run the retrain script, and take a minute to skim its \"help\".\n",
    "\n",
    "```SHELL\n",
    "python -m scripts.retrain -h\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start your retraining with one command. The script downloads the pre-trained model, it adds a new final layer, then trains that layer on the flower photos downloaded.\n",
    "\n",
    "```SHELL\n",
    "python -m scripts.retrain \\\n",
    "  --bottleneck_dir=tf_files/bottlenecks \\\n",
    "  --how_many_training_steps=500 \\\n",
    "  --model_dir=tf_files/models/ \\\n",
    "  --summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\" \\\n",
    "  --output_graph=tf_files/retrained_graph.pb \\\n",
    "  --output_labels=tf_files/retrained_labels.txt \\\n",
    "  --architecture=\"${ARCHITECTURE}\" \\\n",
    "  --image_dir=tf_files/flower_photos\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor is not starting from scratch, it's starting with an existing classifier called *Inception*. *Inception* is one of Google's best image classifiers, and it's open source. This project only has a couple thousand images in the training data, but *Inception* was trained on 1.2 million images from a thousand different categories. TensorFlow for Poets will being with *Inception*, and then use a technique called retraining to adjust it to work with our images. This lets us re-use some of the parameters *Inception* has previously learned so we can create a new high accuracy classifier with far less training data. \n",
    "\n",
    "The classifier only knows about the training data I've shown it. So if I ask it to classify an image, like the Roman COlosseum, it must predict that it's a type of flower. Hopefully, the confidnence will be low.\n",
    "\n",
    "To train a good image classifier, you want DIVERSITY and QUANTITY. There are several hundred images inside the roses folder. This is enough to retrain *Inception*. And you can probalby get away wtih even fewer images, though the accuracy may decrease.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [TensorFlow for Poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0)\n",
    "* [TF Learn](http://tflearn.org/)\n",
    "* [download flowers (*i.e. training images*)]()\n",
    "* [Train Image Classifier with TensorFlow](https://www.youtube.com/watch?v=cSKfRcEDGUs&index=6&list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal)\n",
    "* [tensorflow playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.83261&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
