{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module I'm goign to write my first classifier from scratch. If I can do this on my own, it means I understand an important piece of the puzzle. It means I can write a simple classifier from scratch. \n",
    "\n",
    "The classifier I'm going to write today is a scrappy version of K-Nearest Neighbors. It's one of the simplest classifiers around. Using the code from the previous module, I'm going to comment out the following lines of code and replace it with code I write myself. The rest of the pipeline will stay exactly the same.\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier()\n",
    "```\n",
    "\n",
    "To start, run the original pipeline to see what the accuracy was of the pipeline in the previous module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Module Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_test, X_train, y_test, y_train = train_test_split(X, y, test_size = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this module is over 90%, and this is the goal of the classifier I write myself. So in the next section I'm going to comment out the two lines of the classifier and replace it with code for my own classifier. \n",
    "\n",
    "Right of the bat, this will break my code, so the first thin I need to do is fix the pipeline. To do this, I'll implement a class for my classifier (which I'll call *ScrappyKNN*). By scrappy, I mean bare bones (i.e. just enough to get it working). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Current Module: Get Pipeline Working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to start I'll add some code to the fit() and predict() methods. I'll store the training data in the fit class. You can think of this as just memorizing the data, and you'll see why later on. Inside the predict method, you'll see you need to return a list of predictions. This is because the parameter *X_test* is actually a 2D array, or a list of lists (i.e. each row contains the features for one testing example. To make a prediction for each row, I'll randomly pic a label from the training data and append it to the prediction. At this point the pipeline is working, so lets run it and see how well it does. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# First implement class for my classifier\n",
    "class ScrappyKNN():\n",
    "    # first declare your fit method\n",
    "    # this method takes the features and labels of the training set as input\n",
    "    # so I'm adding parameters for these\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    # as input this receives the features for the testing data\n",
    "    # and as output this it returns predictions for the labels.\n",
    "    # The first goal is to get the pipeline working, and understand whta these methods do\n",
    "    # so before writing the real classifier, let's start with something simpler\n",
    "    # i'll write a random classifier to start... meaning I'll just guess the label...\n",
    "    def predict (self, X_test):\n",
    "        predictions = []\n",
    "        for row in X_test:\n",
    "            # this is randomly pulling a value from y_train and appending it to predictions\n",
    "            label = random.choice(self.y_train)\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_test, X_train, y_test, y_train = train_test_split(X, y, test_size = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out two lines of code from previous module to implement my own classifier\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# classifier = KNeighborsClassifier()\n",
    "\n",
    "# now change my pipeline to use it\n",
    "# looking at the interface for my classifier, there are two I care about...\n",
    "# fit(): fit does the training\n",
    "# predict(): predict does the prediction\n",
    "classifier = ScrappyKNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3466666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, there are three different types of flowers in the iris dataset. So the accuracy should be about 33%, which it is. \n",
    "\n",
    "I know the interface of a classifier, but when I started this exercise, my accuracy was above 90%. So let's see if I can do better than 33%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to kâ€“NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this I'm goning to implment my classifier based on k-nearest neighbors (k-NN). Imagine the dots on the screen are the training data that was memorized in the fit method.\n",
    "\n",
    "![training data](./assets/dostTrainingData.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine, I'm asked to make a prediction on this testing point that is displayed in gray. How can this be done?\n",
    "![training data](./assets/dostTrainingDataGray.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well in the nearest neighbor classifier, it works exaclty as it sounds. By finding the training point that's closest to the testing point, I'll find the nearest neighbor.\n",
    "![training data](./assets/dostTrainingDataGrayKNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I'll predict that the testing point has the same label. For example, in this case I'll guess that the testing dot is *green*, because that's the color of its nearest neighbor.\n",
    "![training data](./assets/dostTrainingDataGreen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example if we had another dot over here...\n",
    "![training data](./assets/dostTrainingDataGray2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd guess that it's red because that's its red.\n",
    "![training data](./assets/dostTrainingDataRed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what about this dot here, right in the middle? Imagine that this dot is equadistant to the nearest green dot and the nearest red dot. There's a tie, so how can I classify it?\n",
    "![training data](./assets/dostTrainingDataGray3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option is I can randomly break the tie. But there's another way, and that's where _K_ comes in. _K_ is the number of neighbors to consider when making your prediction.\n",
    "\n",
    "If _K_ is one, the classifier would only look at the closest neighbor. But if _K_ is 3, the classifier would look at the 3 closest. In this case two of those are green and one is red. To predict, you can vote, and choose the majority class. There is more detail to this algorithm, but that's enough to get started.\n",
    "![training data](./assets/dostTrainingDataKNN3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To code this up, first I'll need a way to find the nearest neighbor. To do this I'll measure the straight line distance between two points, just like you'd do with a rule. There's a formuls for this called the Euclidian distance. \n",
    "![training data](./assets/dostEuclideanDistance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It measures the distance between two points, and it works a bit like the Pythagorean Theorem. And the distance to compute is the length of the hypotenuse.\n",
    "![training data](./assets/dostEuclideanDistance3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now I'm computing distance in two dimensional space brecause I only have two features in the dataset.\n",
    "![training data](./assets/dostDimensions2D.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if I had three features or 3 dimensions? Well, then I'd be in a cube. I could still visualize how to measure distance in this space with a ruler. \n",
    "![training data](./assets/dostDimensions3D.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if I had four features or 4 dimensions (like I do in iris)? Well now I'm in a hyper cube, and I can't measure this very easy.\n",
    "![training data](./assets/dostDimensions4D.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is the Euclidian distance works the same way regardless of the number of dimensions. With more features, I can just more terms to the equation. \n",
    "![training data](./assets/dostEuclideanDistance4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Code Up the Euclidian Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a library called scipy to code up Euclidian distance\n",
    "# scipy is already installed with Anaconda\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# here a and b are lists of numeric features\n",
    "# a is a point from the training data\n",
    "# b is a point from the testing data\n",
    "# this function returns the distance between them\n",
    "def euc(a,b):\n",
    "    return distance.euclidean(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all the math we need, so now let's take a look at the algoriithm for a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Nearest Neighbor Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a prediction for a test point I'll calculate the distance to all the training points. Then I'll predict the testing point has the same label as the closest one.\n",
    "\n",
    "I'll comment out the random prediction I used in the previous example, and replace it with a method that finds the closest training point to the test point. \n",
    "\n",
    "For this example I'll hardcode _K_ equal to 1. So I'm writing a nearest neighbor class. Note: _K_ variable won't appear in the code, since I'm always finding the closest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing class for my classifier\n",
    "class ScrappyKNN():\n",
    "    # first declare your fit method\n",
    "    # this method takes the features and labels of the training set as input\n",
    "    # so I'm adding parameters for these\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    # as input this receives the features for the testing data\n",
    "    # and as output this it returns predictions for the labels.\n",
    "    def predict (self, X_test):\n",
    "        predictions = []\n",
    "        for row in X_test:\n",
    "            # comment out random prediction I made in previous example\n",
    "            # replace it with a method that finds the closest training point to the test point\n",
    "            # label = random.choice(self.y_train)\n",
    "            label = self.closest(row)\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    \n",
    "    # inside this method I'll loop over all the training points\n",
    "    # and keep track of the closest one so far. \n",
    "    # remember the training was memorized in my training function\n",
    "    # and X_train contains the features\n",
    "    def closest(self, row):\n",
    "        # using variable best_dist to keep track of the shortest distance found so far\n",
    "        # calculating distance b/w test point and first training point and \n",
    "        best_dist = euc(row, self.X_train[0])\n",
    "        \n",
    "        # using this variable to keep track of the index of the training point that's closest\n",
    "        # you'll need thsi later to retrieve its label\n",
    "        best_index = 0\n",
    "        \n",
    "        # let's iterate over all the training points\n",
    "        for i in range(1, len(self.X_train)):\n",
    "            dist = euc(row, self.X_train[i])\n",
    "            \n",
    "            # every time I find a closer distance, I'll update the variables\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                \n",
    "                # use the index to return the label for the closest training example\n",
    "                best_index = i\n",
    "        return self.y_train[best_index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " At this point, I have have a working nearest neighbor classifier. So I'm going to run it to see what the accuracy is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_test, X_train, y_test, y_train = train_test_split(X, y, test_size = .5)\n",
    "\n",
    "classifier = ScrappyKNN()\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, I did it. The accuracy is now over 90%. This value may be different with each run the accuracy may be a bit different due to randomness in the train test split. \n",
    "\n",
    "As stated at the very beginning, this is an important milestone. Since I can write this up on my own and understand it, it means I can write a simple classifier from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros\n",
    "* Relativiely simple: easy to understand, and works reasonably well with some problems.\n",
    "\n",
    "\n",
    "### Cons\n",
    "* Computationally intensive: it's slow because it has to iterate over every training point to make a prediction. \n",
    "* Hard to represent relationships between features: some features are more infomative than others, but there's not an easy way to represent that in K-Nearest Neighbors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coming Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the long run I want a classifier that learns mores complex relationships between features, and the label I'm trying to predict. A decision tree is a good example of this and a neural network, like I saw in [tensorflow playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.83261&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) is even better.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Write your first classifier](https://www.youtube.com/watch?v=AoeEHqVSNOw&list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal&index=6&t=0s)\n",
    "* [tensorflow playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.83261&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
